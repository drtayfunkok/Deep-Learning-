{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f41449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network (ANN) machine learning modelleri ile birlikte kullanilir.\n",
    "# Bazi projelerde machine learnin madelleri ile ANN deep learnning modeli kullanmamiz isteyebilirler. \n",
    "# goruntu ile ilgili datalarda convolutional neural network (CNN) kullanilir.\n",
    "# time serilerinde RNN kullanilir. \n",
    "# cok buyuk datalarda ANN daha iyi calisir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870dc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is Deep learning? : Neural network lerin daha deep olanina denir. birden fazla hidden layer olan neural network ler. \n",
    "# ANN machine learning e benziyor. \n",
    "# CNN biraz daha farkli.\n",
    "# bir katman (layer) olursa neural network birden fazla katman (layer) olursa deep learning denir.\n",
    "# layer lar ne kadar fazla olursa model o kadar iyi ogrenir. \n",
    "# deep learnningde data sayisi artikca performans artar. \n",
    "# machine learning modeleri buyuk datalarda performansi cok iyi degildir. \n",
    "# O yuzden buyuk datalarda deep learning tercih edilir. \n",
    "# ANN diger adi Feed forward Network veya Multilayer Perceptron networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0bea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranfer learning : daha once ogrenmis bir modeli aliyoruz ve baska seyler katarak ogrenmesini gelistiriyoruz. \n",
    "# deep learning bize tranfer learning yapmamizi sagliyor. \n",
    "# deep learning in  en onemli avantaji unstructred datalarda (documents, images, text ..) basarili olmasi. \n",
    "# deep learning datadan onemli featurelari aliyor. Buda bize buyuk bir avantaj sagliyor. Feature importance yapmamiza gerek kalmiyor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24995d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bir kutucuga neron neronlarin birlesimine layer denir. \n",
    "# neden bias e ihtiyacimiz var.? \n",
    "# cvp: ağırlıkları değiştirsekte istediğmizi elde edemeyebiliriz ama bias ile bunu sağlayabiliriz\n",
    "# layer sayislarini ve her layerdaki neron sayisini biz kendimiz ayarliyoruz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc61b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function her bir nerondaki degerleri beli bir araliga tasiyor. 0 ile 1 arasindaki gibi. \n",
    "# activation function esnekligi sagliyor.\n",
    "# farkli activation fonksiyonlari var : , binary step function, linear activation function,..\n",
    "# .. sigmoid, softmax, ReLU, TanH, Leaky ReLU.\n",
    "# genelde son katmanda linear activation function kullanilir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87862d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function : 0 ile 1 arasina ceker. islem yuku biraz fazla. \n",
    "# sigmoid functionda vanishing problemi var. \n",
    "# vanishing problem: gittikce inputlarmiizn kuculmesi. buda cok cok kucuk weight anlamina gelir...\n",
    "# buda model katsayilarin (weight) degismesini engelliyor.\n",
    "\n",
    " # ingilizce aciklamasi asagidaki gibi : \n",
    "    \n",
    "# The vanishing gradient problem is caused by the derivative of the activation function used to create..\n",
    "# ..the neural network. The simplest solution to the problem is to replace the activation function of the network.\n",
    "# .. Instead of sigmoid, use an activation function such as ReLU...\n",
    "# .. Rectified Linear Units (ReLU) are activation functions that generate a positive linear output..\n",
    "# ... when they are applied to positive input values. If the input is negative, the function will return zero. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbfaf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TanH en complex activation function\n",
    "# sigmoide gore daha kapsamli. hatta sigmoidden daha iyi. \n",
    "# bundada vanishing problemi olabilir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647652e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu cok kullanilan bir activation function \n",
    "# basit bir activation function\n",
    "# hesaplama maliyeti cok dusuk. negatif yani. \n",
    "# bundada dying (tamamen inputlari oldurme) problemi var. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2099d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLu dying problemini halletmek icin bir activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375dcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax multiclasification problemlerinde kullanilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38eb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost funtion (lost function) : maliyet fonksiyonu demek. amacimiz cost function i minimize etmek. \n",
    "# cost function : gercek deger ile tahmin edilen degerin  farkindan elde edilen degerin toplaminin ortalamasi. \n",
    "# hata payim demek aslinda. \n",
    "# her bir hataya lost, bunlarin toplamin ortalamisina cost function denir.\n",
    "# cost function i minimize etmek icin weight leri optimize ediyoruz bunuda gradient decsent ile yapacagiz. \n",
    "# cost function J ile gosterilir.\n",
    "# data setini deep learningte uce bolecegiz. bunlar train, validation ve test dataset. \n",
    "# validation dataset icin bunu model icinde validation_split kullaniyoruz. \n",
    "# validation_split=0.1 ise bu train datasinin yuzde 10 nu validation icin kullan anlamina gelir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bee2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learningde de overfiting underfitting durumu oluyor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce6110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification icin output bir tanedir.\n",
    "# multiclass clasification icin ourput degeri degisir.3 sinif ise 3 output olur gibi. \n",
    "# multiclass ikiye ayrilir :  Mutually exclusive classes (bir tane cikti var, red green gibi),..\n",
    "# .. digeride Non exclusive classes (iki tane cikti var (A,B) gibi). \n",
    "# daha cok  Mutually exclusive classes kullanacagiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7253371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
